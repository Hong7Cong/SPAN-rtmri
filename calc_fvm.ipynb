{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongn/miniconda3/envs/genai/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet3D, ElucidatedImagen, ImagenTrainer\n",
    "from utils import gif75speaker, get_path_of_pretrained\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "# import argparse\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create generated dataset to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/PCM/Documents/GitHub/SPAN-rtmri/checkpoints/hubert/large/ImagenVideo-Modelhubert-large-PoolingFalse-IgnoreTimeFalse-TwoStepTrue-100\n"
     ]
    }
   ],
   "source": [
    "AUDIO_EMB = 'hubert-large'\n",
    "POOLING = False\n",
    "MODE = 'test-unseenboth' #Select {test-unseenaudio, test-unseensubject, test-unseenboth}\n",
    "LEN_GEN_IMGS = 500 # Number of generated images for evaluation\n",
    "PATH_2_PRETRAINED, LEN_AUDIO_EMB = get_path_of_pretrained(AUDIO_EMB, POOLING)\n",
    "print(PATH_2_PRETRAINED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_75speaker = gif75speaker(image_path = './datasets/preprocessed_dataset/test', \n",
    "                                img_per_gif = 10, \n",
    "                                audio_path = f'./datasets/preprocessed_dataset/{AUDIO_EMB}', \n",
    "                                audio_pooling = POOLING,\n",
    "                                mode = MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet1 = Unet3D(dim = 64, dim_mults = (1, 2, 4, 8)).cuda()\n",
    "unet2 = Unet3D(dim = 64, dim_mults = (1, 2, 4, 8)).cuda()\n",
    "\n",
    "imagen = ElucidatedImagen(\n",
    "    text_embed_dim = LEN_AUDIO_EMB,\n",
    "    unets = (unet1, unet2),\n",
    "    image_sizes = (64, 64),\n",
    "    random_crop_sizes = (None, 16),\n",
    "    temporal_downsample_factor = (1, 1),        # in this example, the first unet would receive the video temporally downsampled by 2x\n",
    "    num_sample_steps = 10,\n",
    "    cond_drop_prob = 0.1,\n",
    "    sigma_min = 0.002,                          # min noise level\n",
    "    sigma_max = (80, 160),                      # max noise level, double the max noise level for upsampler\n",
    "    sigma_data = 0.5,                           # standard deviation of data distribution\n",
    "    rho = 7,                                    # controls the sampling schedule\n",
    "    P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n",
    "    P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n",
    "    S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n",
    "    S_tmin = 0.05,\n",
    "    S_tmax = 50,\n",
    "    S_noise = 1.003,\n",
    ").cuda()\n",
    "\n",
    "imagen.load_state_dict(torch.load(PATH_2_PRETRAINED))\n",
    "trainer = ImagenTrainer(imagen,\n",
    "    split_valid_from_train = True, # whether to split the validation dataset from the training\n",
    "    dl_tuple_output_keywords_names = ('images', 'text_embeds', 'cond_video_frames')\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "# !rm -rf ./generated_images\n",
    "# !mkdir ./generated_images\n",
    "# subprocess.call(f\"rm -rf ./generated_images/{AUDIO_EMB}\", shell=True)\n",
    "# subprocess.call(f\"mkdir ./generated_images/{AUDIO_EMB}\", shell=True)\n",
    "subprocess.call(f\"rm -rf ./generated_images/{AUDIO_EMB}/{MODE}\", shell=True)\n",
    "subprocess.call(f\"mkdir ./generated_images/{AUDIO_EMB}/{MODE}\", shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub067_2drt_21_topic5_video-1180\n",
      "unet 1 has not been trained\n",
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0038161277770996094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3830f96bc19a453db6524ec967fd5d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0037529468536376953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "sampling time step",
       "rate": null,
       "total": 10,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a5d0b437ff4334a282696becda9cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling time step:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# real_path = []\n",
    "\n",
    "for i in range(LEN_GEN_IMGS):\n",
    "    (_, aud_emb, cond_video_frames) = dataset_75speaker[i]\n",
    "    # real_path.append(dataset_75speaker.get_path(i))\n",
    "    print(f'{dataset_75speaker.get_names(i)}')\n",
    "    videos = trainer.sample(text_embeds = aud_emb.unsqueeze(0), video_frames = 10, stop_at_unet_number  = 1, batch_size = 1, cond_video_frames=cond_video_frames.unsqueeze(0))\n",
    "    imgs = torch.transpose(videos[0], 0, 1)\n",
    "    imgs = [transforms.ToPILImage()(img) for img in imgs]\n",
    "    # duration is the number of milliseconds between frames; this is 40 frames per second\n",
    "    # model_name = opt.audio_path.split('/')[-1]\n",
    "    imgs[0].save(f'./generated_images/{AUDIO_EMB}/{MODE}/{dataset_75speaker.get_names(i)}.gif', save_all=True, append_images=imgs[1:], duration=10, loop=0)\n",
    "\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc FVD for UNSEEN SUBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image, ImageSequence\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_frames(image: Image, mode='RGB'):\n",
    "    # ret = \n",
    "    # if self.transform:\n",
    "    #     gif = self.transform(gif)\n",
    "    return np.array([\n",
    "        np.array(frame.convert(mode))\n",
    "        for frame in ImageSequence.Iterator(image)\n",
    "    ])\n",
    "\n",
    "def load_frames_tensor(im: Image, mode='RGB', video_len=10):\n",
    "    return torch.stack([transforms.ToTensor()(np.array(frame.convert('RGB'))) for frame in ImageSequence.Iterator(im)])[:video_len]\n",
    "\n",
    "def get_videos_from_folder(paths):\n",
    "    synthetic_batch = []\n",
    "    for names in paths:\n",
    "        with Image.open(names) as im:\n",
    "            gif = load_frames_tensor(im)\n",
    "            # gif = load_frames_tensor(im)\n",
    "            synthetic_batch.append(gif)\n",
    "    synthetic_batch = torch.stack(synthetic_batch)\n",
    "    return synthetic_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_batch = get_videos_from_folder(glob.glob(f'./generated_images/{AUDIO_EMB}/{MODE}/*')[:LEN_GEN_IMGS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test_paths = ['./datasets/preprocessed_dataset/test/' + name.split('/')[-1] for name in glob.glob(f'./generated_images/{AUDIO_EMB}/{MODE}/*')[:LEN_GEN_IMGS]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch_test = get_videos_from_folder(real_test_paths)\n",
    "real_batch_train = get_videos_from_folder(glob.glob(f'./datasets/preprocessed_dataset/train/*')[:LEN_GEN_IMGS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic_batch = []\n",
    "# synthetic_path = glob.glob(f'./generated_images/*')[:300]\n",
    "# for names in synthetic_path:\n",
    "#     with Image.open(names) as im:\n",
    "#         gif = load_frames_tensor(im)\n",
    "#         # gif = load_frames_tensor(im)\n",
    "#         synthetic_batch.append(gif)\n",
    "# synthetic_batch = torch.stack(synthetic_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_path = real_batch#glob.glob(f'./datasets/preprocessed_dataset/test/*')[:300]\n",
    "# real_batch = []\n",
    "# for names in real_path[:300]:\n",
    "#     with Image.open(names) as im:\n",
    "#         gif = load_frames_tensor(im)\n",
    "#         # gif = load_frames_tensor(im)\n",
    "#         real_batch.append(gif)\n",
    "# real_batch = torch.stack(real_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_batch2 = []\n",
    "# real_path2 = glob.glob(f'./datasets/preprocessed_dataset/train/*')[:300]\n",
    "# for names in real_path2:\n",
    "#     with Image.open(names) as im:\n",
    "#         gif = load_frames_tensor(im)\n",
    "#         # gif = load_frames_tensor(im)\n",
    "#         real_batch2.append(gif)\n",
    "# real_batch2 = torch.stack(real_batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "# appending a path\n",
    "sys.path.append('common_metrics_on_video_quality')\n",
    "from calculate_fvd import calculate_fvd\n",
    "from calculate_psnr import calculate_psnr\n",
    "from calculate_ssim import calculate_ssim\n",
    "from calculate_lpips import calculate_lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_fvd...\n",
      "/mnt/c/Users/PCM/Documents/GitHub/SPAN-rtmri/common_metrics_on_video_quality/fvd/styleganv/i3d_torchscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_fvd...\n",
      "/mnt/c/Users/PCM/Documents/GitHub/SPAN-rtmri/common_metrics_on_video_quality/fvd/styleganv/i3d_torchscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:09<00:00,  9.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:04<00:00, 104.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_psnr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 3227.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:04<00:00, 105.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_psnr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 3320.14it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "import json\n",
    "result = {}\n",
    "result['fvd_realvsfake'] = calculate_fvd(synthetic_batch, real_batch_train, device, method='styleganv')\n",
    "result['fvd_realvsreal'] = calculate_fvd(real_batch_test, real_batch_train, device, method='styleganv')\n",
    "result['ssim_realvsfake'] = calculate_ssim(synthetic_batch, real_batch_train)\n",
    "result['psnr_realvsfake'] = calculate_psnr(synthetic_batch, real_batch_train)\n",
    "result['ssim_realvsreal'] = calculate_ssim(real_batch_test, real_batch_train)\n",
    "result['psnr_realvsreal'] = calculate_psnr(real_batch_test, real_batch_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1108799424802223"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(result['ssim_realvsfake']['value'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26404798849270844"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(result['ssim_realvsreal']['value'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {10: 209.97105881616136},\n",
       " 'video_setting': torch.Size([500, 3, 10, 64, 64]),\n",
       " 'video_setting_name': 'batch_size, channel, time, heigth, width'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['fvd_realvsreal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {10: 1673.3681691459751},\n",
       " 'video_setting': torch.Size([500, 3, 10, 64, 64]),\n",
       " 'video_setting_name': 'batch_size, channel, time, heigth, width'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['fvd_realvsfake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc FVD for 2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import glob\n",
    "def get_length(filename):\n",
    "    result = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n",
    "                             \"format=duration\", \"-of\",\n",
    "                             \"default=noprint_wrappers=1:nokey=1\", filename],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT)\n",
    "    return float(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKE TEST DATASET\n",
    "subjects = glob.glob(f'/mnt/c/Users/PCM/Dropbox/span/sub*')[60:]\n",
    "for sub in subjects:\n",
    "    vids = glob.glob(f'{sub}/2drt/video/*')\n",
    "    window = 0.4 # step = window - overlap\n",
    "    overlap = 0.2\n",
    "    for i in range(len(vids)):\n",
    "        for skip in np.arange(0, int(get_length(vids[i]))-1, window-overlap):\n",
    "            command = f\"ffmpeg -y -ss {skip} -t {window} -i {vids[i]} -vf \\\"fps=50,scale=64:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\\\" -loop 0 ./datasets/preprocessed_dataset/test-2/{vids[i].split('/')[-1].split('.')[0]}-{int(skip*50)}.gif\"\n",
    "            subprocess.call(command, shell=True)\n",
    "\n",
    "subjects = glob.glob(f'/mnt/c/Users/PCM/Dropbox/span/sub*')[:60]\n",
    "for sub in subjects:\n",
    "    vids = glob.glob(f'{sub}/2drt/video/*')[28:]\n",
    "    window = 0.4 # step = window - overlap\n",
    "    overlap = 0.2\n",
    "    for i in range(len(vids)):\n",
    "        for skip in np.arange(0, int(get_length(vids[i]))-1, window-overlap):\n",
    "            command = f\"ffmpeg -y -ss {skip} -t {window} -i {vids[i]} -vf \\\"fps=50,scale=64:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\\\" -loop 0 ./datasets/preprocessed_dataset/test-2/{vids[i].split('/')[-1].split('.')[0]}-{int(skip*50)}.gif\"\n",
    "            subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "subjects = glob.glob(f'/mnt/c/Users/PCM/Dropbox/span/sub*')[:60]\n",
    "for sub in subjects:\n",
    "    vids = glob.glob(f'{sub}/2drt/video/*')[:28]\n",
    "    # vids = glob.glob(f'/mnt/c/Users/PCM/Dropbox/span/sub006/2drt/video/*')\n",
    "    window = 1 # step = window - overlap\n",
    "    overlap = 0\n",
    "    for i in range(len(vids)):\n",
    "        for skip in np.arange(0, int(get_length(vids[i]))-1, window-overlap):\n",
    "            command = f\"ffmpeg -y -ss {skip} -t {window} -i {vids[i]} -vf \\\"fps=50,scale=64:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\\\" -loop 0 ./datasets/preprocessed_dataset/train-1/{vids[i].split('/')[-1].split('.')[0]}-{int(skip*50)}.gif\"\n",
    "            subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./generated_images_1s\n",
    "!mkdir ./generated_images_1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_path = []\n",
    "\n",
    "for i in range(100):\n",
    "    (_, aud_emb, preceding) = dataset_75speaker[i]\n",
    "    real_path.append(dataset_75speaker.get_path(i))\n",
    "    print(f'{dataset_75speaker.get_names(i)}')\n",
    "\n",
    "    aud_embs = dataset_75speaker.get_audio_emb(i)\n",
    "    start_frame = dataset_75speaker.get_start_index(i)\n",
    "    listimgs = []\n",
    "    MILISECOND = 1\n",
    "    for i in range(start_frame,start_frame + int(MILISECOND/0.02), 10):\n",
    "        print(i)\n",
    "        aud_emb = torch.mean(aud_embs[:,i:i+10,:], axis=1).unsqueeze(0) #aud_embs[:,i:i+10,:]#\n",
    "        sample_img = imagen.sample(text_embeds = aud_emb.cuda(), video_frames = 10, stop_at_unet_number=1, skip_steps=0, cond_video_frames = preceding.unsqueeze(0).cuda())\n",
    "        preceding = sample_img[0,:,-1:,:]\n",
    "        # init_images = add_noise_video(sample_img, image_sizes = (64, 64), timesteps=1000, times = 300).cuda()\n",
    "        listimgs.append(sample_img)\n",
    "    # videos = trainer.sample(text_embeds = aud_emb.unsqueeze(0), video_frames = 10, stop_at_unet_number  = 1, batch_size = 1, cond_video_frames=cond_video_frames.unsqueeze(0))\n",
    "    # imgs = torch.transpose(videos[0], 0, 1)\n",
    "    # imgs = [transforms.ToPILImage()(img) for img in imgs]\n",
    "    # duration is the number of milliseconds between frames; this is 40 frames per second\n",
    "    # model_name = opt.audio_path.split('/')[-1]\n",
    "    # imgs[0].save(f'./generated_images_1s/{dataset_75speaker.get_names(i)}.gif', save_all=True, append_images=imgs[1:], duration=10, loop=0)\n",
    "    a = torch.transpose(torch.stack(listimgs, axis=0).squeeze(1),1,2)\n",
    "    b = a.reshape(int(MILISECOND/0.02), 3, 64, 64)\n",
    "    imgs = [transforms.ToPILImage()(img) for img in b]\n",
    "    # duration is the number of milliseconds between frames; this is 40 frames per second\n",
    "    imgs[0].save(f'./generated_images_1s/{dataset_75speaker.get_names(i)}.gif', save_all=True, append_images=imgs[1:], duration=10, loop=0)\n",
    "\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.transpose(torch.stack(listimgs, axis=0).squeeze(1),1,2)\n",
    "b = a.reshape(200,3,64,64)\n",
    "imgs = [transforms.ToPILImage()(img) for img in b]\n",
    "# duration is the number of milliseconds between frames; this is 40 frames per second\n",
    "imgs[0].save(f'gif-sample-video.gif', save_all=True, append_images=imgs[1:], duration=20, loop=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
