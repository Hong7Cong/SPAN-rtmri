{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video to gif preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongn/miniconda3/envs/genai/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import glob, torch\n",
    "import subprocess\n",
    "from utils import *\n",
    "from PIL import Image, ImageSequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_opencv(filename):\n",
    "    import cv2\n",
    "    video = cv2.VideoCapture(filename)\n",
    "\n",
    "    duration = video.get(cv2.CAP_PROP_POS_MSEC)\n",
    "    frame_count = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "    return duration, frame_count\n",
    "\n",
    "def get_length(filename):\n",
    "    result = subprocess.run([\"ffprobe\", \"-v\", \"error\", \"-show_entries\",\n",
    "                             \"format=duration\", \"-of\",\n",
    "                             \"default=noprint_wrappers=1:nokey=1\", filename],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT)\n",
    "    return float(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make video into gifs with fps=50, length = window = 0.5s, no overlap (overlap=0)\n",
    "!rm -rf ./datasets/gifs-84/\n",
    "!mkdir ./datasets/gifs-84/\n",
    "\n",
    "target_dir = './datasets/gifs'\n",
    "subjects = glob.glob('/mnt/c/Users/PCM/Dropbox/span/sub*')\n",
    "for sub in subjects:\n",
    "    vids = glob.glob(f'{sub}/2drt/video/*')\n",
    "    # vids = glob.glob(f'/mnt/c/Users/PCM/Dropbox/span/sub006/2drt/video/*')\n",
    "    window = 0.2 # step = window - overlap\n",
    "    overlap = 0\n",
    "    for i in range(len(vids)):\n",
    "        for skip in np.arange(0, int(get_length(vids[i]))-1, window-overlap):\n",
    "            command = f\"ffmpeg -y -ss {skip} -t {window} -i {vids[i]} -vf \\\"fps=50,scale=64:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\\\" -loop 0 ./datasets/gifs/{vids[i].split('/')[-1].split('.')[0]}-{int(skip*50)}.gif\"\n",
    "            subprocess.call(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import glob, torch\n",
    "\n",
    "class gif75speaker(Dataset):\n",
    "    def __init__(self, image_path = './datasets/gifs', audio_path = './datasets/audios', transform=None, target_transform=None, img_per_gif = 10):\n",
    "        self.gifs = glob.glob(f'{image_path}/*')  # Could be a list: ['./train/input/image_1.bmp', './train/input/image_2.bmp', ...]\n",
    "        # self.audios = glob.glob(f'{audio_path}/*')  # Could be a nested list: [['./train/GT/image_1_1.bmp', './train/GT/image_1_2.bmp', ...], ['./train/GT/image_2_1.bmp', './train/GT/image_2_2.bmp', ...]]\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.img_per_gif = img_per_gif\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        gifs_name = self.gifs[index].split('/')[-1].split('.')[0].split('-')\n",
    "\n",
    "        with Image.open(self.gifs[index]) as im:\n",
    "            gif = self.load_frames(im)\n",
    "        # gif = Image.open(self.images[index])\n",
    "\n",
    "        aud_embs = torch.load(f'./datasets/audios/{gifs_name[0]}.pt')\n",
    "        aud_emb = aud_embs[:,int(gifs_name[-1]):int(gifs_name[-1]) + self.img_per_gif,:]\n",
    "        gif = torch.transpose(torch.stack([transforms.ToTensor()(i) for i in gif[:self.img_per_gif]]), 0,1)\n",
    "        return (gif, aud_emb[0], gif[:,0:2,:])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.gifs)\n",
    "    \n",
    "    def load_frames(self, image: Image, mode='RGB'):\n",
    "        # ret = \n",
    "        # if self.transform:\n",
    "        #     gif = self.transform(gif)\n",
    "        return np.array([\n",
    "            np.array(frame.convert(mode))\n",
    "            for frame in ImageSequence.Iterator(image)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(gifs, aud_emb, preceding_frame) = next(iter(gif75speaker(transform=data_transforms['val'], img_per_gif=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 64, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preceding_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gif75speaker())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image, ImageSequence\n",
    "# import numpy as np\n",
    "\n",
    "# def load_frames(image: Image, mode='RGB'):\n",
    "#     return np.array([\n",
    "#         np.array(frame.convert(mode))\n",
    "#         for frame in ImageSequence.Iterator(image)\n",
    "#     ])\n",
    "# gifs_name = '/mnt/c/Users/PCM/Documents/GitHub/SPAN-rtmri/datasets/gifs/sub006_2drt_20_topic4_video-550.gif'\n",
    "# with Image.open(gifs_name) as im:\n",
    "#     frames = load_frames(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(gif75speaker(), batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet3D, ElucidatedImagen, ImagenTrainer\n",
    "\n",
    "unet1 = Unet3D(dim = 64, dim_mults = (1, 2, 4, 8)).cuda()\n",
    "\n",
    "unet2 = Unet3D(dim = 64, dim_mults = (1, 2, 4, 8)).cuda()\n",
    "\n",
    "# elucidated imagen, which contains the unets above (base unet and super resoluting ones)\n",
    "\n",
    "imagen = ElucidatedImagen(\n",
    "    text_embed_dim=1024,\n",
    "    unets = (unet1, unet2),\n",
    "    image_sizes = (64, 64),\n",
    "    random_crop_sizes = (None, 16),\n",
    "    temporal_downsample_factor = (2, 1),        # in this example, the first unet would receive the video temporally downsampled by 2x\n",
    "    num_sample_steps = 10,\n",
    "    cond_drop_prob = 0.1,\n",
    "    sigma_min = 0.002,                          # min noise level\n",
    "    sigma_max = (80, 160),                      # max noise level, double the max noise level for upsampler\n",
    "    sigma_data = 0.5,                           # standard deviation of data distribution\n",
    "    rho = 7,                                    # controls the sampling schedule\n",
    "    P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n",
    "    P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n",
    "    S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n",
    "    S_tmin = 0.05,\n",
    "    S_tmax = 50,\n",
    "    S_noise = 1.003,\n",
    ").cuda()\n",
    "\n",
    "# mock videos (get a lot of this) and text encodings from large T5\n",
    "\n",
    "texts = [\n",
    "    'a whale breaching from afar',\n",
    "    'young girl blowing out candles on her birthday cake',\n",
    "    'fireworks with blue and green sparkles',\n",
    "    'dust motes swirling in the morning sunshine on the windowsill'\n",
    "]\n",
    "\n",
    "# videos = torch.randn(4, 3, 10, 32, 32).cuda() # (batch, channels, time / video frames, height, width)\n",
    "\n",
    "# feed images into imagen, training each unet in the cascade\n",
    "# for this example, only training unet 1\n",
    "\n",
    "trainer = ImagenTrainer(imagen,\n",
    "    split_valid_from_train = True, # whether to split the validation dataset from the training\n",
    "    dl_tuple_output_keywords_names = ('images', 'text_embeds', 'cond_video_frames')\n",
    ").cuda()\n",
    "\n",
    "# you can also ignore time when training on video initially, shown to improve results in video-ddpm paper. eventually will make the 3d unet trainable with either images or video. research shows it is essential (with current data regimes) to train first on text-to-image. probably won't be true in another decade. all big data becomes small data\n",
    "# for i in range(1,200000):\n",
    "trainer.add_train_dataset(gif75speaker(), batch_size = 8)\n",
    "\n",
    "for i in range(1,20000):\n",
    "    loss = trainer.train_step(unet_number = 1, max_batch_size = 8, ignore_time = False)\n",
    "    print(f'loss: {loss}')\n",
    "\n",
    "    if not (i % 50):\n",
    "        valid_loss = trainer.valid_step(unet_number = 1, max_batch_size = 8)\n",
    "        print(f'valid loss: {valid_loss}')\n",
    "\n",
    "    if not (i % 100) and trainer.is_main: # is_main makes sure this can run in distributed\n",
    "        videos = trainer.sample(text_embeds = aud_emb.unsqueeze(0), video_frames = 10, stop_at_unet_number  = 1, batch_size = 1)\n",
    "        imgs = torch.transpose(videos[0], 0, 1)\n",
    "        imgs = [transforms.ToPILImage()(img) for img in imgs]\n",
    "        # duration is the number of milliseconds between frames; this is 40 frames per second\n",
    "        # imgs[0].save(f'./gif_samples/gif-sample-{i // 100}.gif', save_all=True, append_images=imgs[1:], duration=20, loop=0)\n",
    "        # torch.save(imagen.state_dict(), f'./checkpoints/imagen-video-{i}')\n",
    "\n",
    "# losses = 0\n",
    "# for (videos, aud_emb) in train_dataloader:\n",
    "#     trainer(videos, text_embeds = aud_emb, unet_number = 1, ignore_time = False)\n",
    "#     trainer.update(unet_number = 1)\n",
    "\n",
    "    # if not (i % 5):\n",
    "    #     valid_loss = trainer.valid_step(unet_number = 1, max_batch_size = 32)\n",
    "    #     print(f'valid loss: {valid_loss}')\n",
    "\n",
    "    # if not (i % 10) and trainer.is_main: # is_main makes sure this can run in distributed\n",
    "    #     videos = trainer.sample(text_embeds = aud_emb, video_frames = 20, stop_at_unet_number  = 1, batch_size = 1) # returns List[Image]\n",
    "    #     images[0].save(f'./sample_log/sample-{i // 100}.png')\n",
    "\n",
    "# videos = trainer.sample(texts = texts, video_frames = 20) # extrapolating to 20 frames from training on 10 frames\n",
    "\n",
    "# videos.shape # (4, 3, 20, 32, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imagen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_301/1630282939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimagen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'imagen' is not defined"
     ]
    }
   ],
   "source": [
    "imagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = trainer.sample(text_embeds = aud_emb, video_frames = 20, stop_at_unet_number  = 1, batch_size = 1) # extrapolating to 20 frames from training on 10 frames\n",
    "videos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.to_pil_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "imgs = torch.transpose(videos[0], 0, 1)\n",
    "imgs = [transforms.ToPILImage()(img) for img in imgs]\n",
    "# duration is the number of milliseconds between frames; this is 40 frames per second\n",
    "imgs[0].save(\"sample-imagen-video.gif\", save_all=True, append_images=imgs[1:], duration=20, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.transpose(videos[0], 0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
