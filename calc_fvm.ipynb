{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongn/miniconda3/envs/genai/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from imagen_pytorch import Unet3D, ElucidatedImagen, ImagenTrainer\n",
    "from utils import gif75speaker, get_path_of_pretrained\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "# import argparse\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create generated dataset to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_EMB = 'wav2vec2-l60-pho'\n",
    "POOLING = False\n",
    "MODE = 'test-unseenboth' #Select {test-unseenaudio, test-unseensubject, test-unseenboth}\n",
    "LEN_GEN_IMGS = 300 # Number of generated images for evaluation\n",
    "PATH_2_PRETRAINED, LEN_AUDIO_EMB = get_path_of_pretrained(AUDIO_EMB, POOLING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_75speaker = gif75speaker(image_path = './datasets/preprocessed_dataset/test', \n",
    "                                img_per_gif = 10, \n",
    "                                audio_path = f'./datasets/preprocessed_dataset/{AUDIO_EMB}', \n",
    "                                audio_pooling = POOLING,\n",
    "                                mode = MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hongn/miniconda3/envs/genai/lib/python3.8/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(split_batches=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "unet1 = Unet3D(dim = 64, dim_mults = (1, 2, 4, 8)).cuda()\n",
    "unet2 = Unet3D(dim = 64, dim_mults = (1, 2, 4, 8)).cuda()\n",
    "\n",
    "imagen = ElucidatedImagen(\n",
    "    text_embed_dim = LEN_AUDIO_EMB,\n",
    "    unets = (unet1, unet2),\n",
    "    image_sizes = (64, 64),\n",
    "    random_crop_sizes = (None, 16),\n",
    "    temporal_downsample_factor = (1, 1),        # in this example, the first unet would receive the video temporally downsampled by 2x\n",
    "    num_sample_steps = 10,\n",
    "    cond_drop_prob = 0.1,\n",
    "    sigma_min = 0.002,                          # min noise level\n",
    "    sigma_max = (80, 160),                      # max noise level, double the max noise level for upsampler\n",
    "    sigma_data = 0.5,                           # standard deviation of data distribution\n",
    "    rho = 7,                                    # controls the sampling schedule\n",
    "    P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n",
    "    P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n",
    "    S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n",
    "    S_tmin = 0.05,\n",
    "    S_tmax = 50,\n",
    "    S_noise = 1.003,\n",
    ").cuda()\n",
    "\n",
    "imagen.load_state_dict(torch.load(PATH_2_PRETRAINED))\n",
    "trainer = ImagenTrainer(imagen,\n",
    "    split_valid_from_train = True, # whether to split the validation dataset from the training\n",
    "    dl_tuple_output_keywords_names = ('images', 'text_embeds', 'cond_video_frames')\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./generated_images\n",
    "!mkdir ./generated_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_path = []\n",
    "\n",
    "for i in range(301):\n",
    "    (_, aud_emb, cond_video_frames) = dataset_75speaker[i]\n",
    "    real_path.append(dataset_75speaker.get_path(i))\n",
    "    print(f'{dataset_75speaker.get_names(i)}')\n",
    "    videos = trainer.sample(text_embeds = aud_emb.unsqueeze(0), video_frames = 10, stop_at_unet_number  = 1, batch_size = 1, cond_video_frames=cond_video_frames.unsqueeze(0))\n",
    "    imgs = torch.transpose(videos[0], 0, 1)\n",
    "    imgs = [transforms.ToPILImage()(img) for img in imgs]\n",
    "    # duration is the number of milliseconds between frames; this is 40 frames per second\n",
    "    # model_name = opt.audio_path.split('/')[-1]\n",
    "    imgs[0].save(f'./generated_images/{dataset_75speaker.get_names(i)}.gif', save_all=True, append_images=imgs[1:], duration=10, loop=0)\n",
    "\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc FVD for UNSEEN SUBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image, ImageSequence\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_frames(image: Image, mode='RGB'):\n",
    "    # ret = \n",
    "    # if self.transform:\n",
    "    #     gif = self.transform(gif)\n",
    "    return np.array([\n",
    "        np.array(frame.convert(mode))\n",
    "        for frame in ImageSequence.Iterator(image)\n",
    "    ])\n",
    "\n",
    "def load_frames_tensor(image: Image, mode='RGB', video_len=10):\n",
    "    return torch.stack([transforms.ToTensor()(np.array(frame.convert('RGB'))) for frame in ImageSequence.Iterator(im)])[:video_len]\n",
    "\n",
    "# def get_videos_from_folder(path, size_batch):\n",
    "#     synthetic_batch = []\n",
    "#     print(f'{path}/*')\n",
    "#     synthetic_path = glob.glob(path + '/*')[:10]\n",
    "#     for names in synthetic_path:\n",
    "#         with Image.open(names) as im:\n",
    "#             gif = load_frames_tensor(im)\n",
    "#             synthetic_batch.append(gif)\n",
    "#     synthetic_batch = torch.stack(synthetic_batch)\n",
    "#     return synthetic_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_batch = []\n",
    "synthetic_path = glob.glob(f'./generated_images/*')[:300]\n",
    "for names in synthetic_path:\n",
    "    with Image.open(names) as im:\n",
    "        gif = load_frames_tensor(im)\n",
    "        # gif = load_frames_tensor(im)\n",
    "        synthetic_batch.append(gif)\n",
    "synthetic_batch = torch.stack(synthetic_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real_path = real_batch#glob.glob(f'./datasets/preprocessed_dataset/test/*')[:300]\n",
    "real_batch = []\n",
    "for names in real_path[:300]:\n",
    "    with Image.open(names) as im:\n",
    "        gif = load_frames_tensor(im)\n",
    "        # gif = load_frames_tensor(im)\n",
    "        real_batch.append(gif)\n",
    "real_batch = torch.stack(real_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_batch2 = []\n",
    "real_path2 = glob.glob(f'./datasets/preprocessed_dataset/train/*')[:300]\n",
    "for names in real_path2:\n",
    "    with Image.open(names) as im:\n",
    "        gif = load_frames_tensor(im)\n",
    "        # gif = load_frames_tensor(im)\n",
    "        real_batch2.append(gif)\n",
    "real_batch2 = torch.stack(real_batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "# appending a path\n",
    "sys.path.append('common_metrics_on_video_quality')\n",
    "from calculate_fvd import calculate_fvd\n",
    "from calculate_psnr import calculate_psnr\n",
    "from calculate_ssim import calculate_ssim\n",
    "from calculate_lpips import calculate_lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_fvd...\n",
      "/mnt/c/Users/PCM/Documents/GitHub/SPAN-rtmri/common_metrics_on_video_quality/fvd/styleganv/i3d_torchscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:30<00:00, 30.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_fvd...\n",
      "/mnt/c/Users/PCM/Documents/GitHub/SPAN-rtmri/common_metrics_on_video_quality/fvd/styleganv/i3d_torchscript.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:24<00:00, 24.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:08<00:00, 34.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_psnr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:01<00:00, 274.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_ssim...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:09<00:00, 30.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculate_psnr...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 1003.12it/s]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "import json\n",
    "result = {}\n",
    "result['fvd_realvsfake'] = calculate_fvd(synthetic_batch, real_batch2, device, method='styleganv')\n",
    "result['fvd_realvsreal'] = calculate_fvd(real_batch, real_batch2, device, method='styleganv')\n",
    "result['ssim_realvsfake'] = calculate_ssim(synthetic_batch, real_batch2)\n",
    "result['psnr_realvsfake'] = calculate_psnr(synthetic_batch, real_batch2)\n",
    "result['ssim_realvsreal'] = calculate_ssim(real_batch, real_batch2)\n",
    "result['psnr_realvsreal'] = calculate_psnr(real_batch, real_batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14528324416866542"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(result['ssim_realvsfake']['value'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3241815948654033"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(result['ssim_realvsreal']['value'].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {10: 295.53774701782646},\n",
       " 'video_setting': torch.Size([300, 3, 10, 64, 64]),\n",
       " 'video_setting_name': 'batch_size, channel, time, heigth, width'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['fvd_realvsreal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': {10: 1701.0833649052056},\n",
       " 'video_setting': torch.Size([300, 3, 10, 64, 64]),\n",
       " 'video_setting_name': 'batch_size, channel, time, heigth, width'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['fvd_realvsfake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 64, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gif.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc FVD for 2 seconds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
