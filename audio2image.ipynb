{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int(glob.glob('./datasets/images/*')[0].split('/')[-1].split('.')[0].split('_')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(glob.glob('./datasets/images/*')[0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(64),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.CenterCrop(64),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean, std)\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "\n",
    "class span75speaker(Dataset):\n",
    "    def __init__(self, image_path = './datasets/images', audio_path = './datasets/audios', transform=None, target_transform=None):\n",
    "        self.images = glob.glob(f'{image_path}/*')  # Could be a list: ['./train/input/image_1.bmp', './train/input/image_2.bmp', ...]\n",
    "        # self.audios = glob.glob(f'{audio_path}/*')  # Could be a nested list: [['./train/GT/image_1_1.bmp', './train/GT/image_1_2.bmp', ...], ['./train/GT/image_2_1.bmp', './train/GT/image_2_2.bmp', ...]]\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.images[index].split('/')[-1].split('.')[0].split('-')\n",
    "        img = Image.open(self.images[index])\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        aud_embs = torch.load(f'./datasets/audios/{image_name[0]}.pt')\n",
    "        aud_emb = aud_embs[:,min(int(image_name[-1]), aud_embs.size(1) - 1),:]\n",
    "\n",
    "        return (img, aud_emb)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = span75speaker(transform = data_transforms['val'])\n",
    "(img, aud) = next(iter(traindata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import torch\n",
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load model directly\n",
    "# from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "# processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "# modelphonene = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelphonene = model\n",
    "modelphonene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import librosa\n",
    "# import torch\n",
    "# from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2Model\n",
    "\n",
    "input_audio, sample_rate = librosa.load(\"/mnt/c/Users/PCM/Dropbox/span/sub006/2drt/audio/sub006_2drt_01_vcv1_r1_video.wav\",  sr=16000)\n",
    "\n",
    "model_name = \"facebook/wav2vec2-lv-60-espeak-cv-ft\"#\"facebook/wav2vec2-base-960h\"#\"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "i= feature_extractor(input_audio, return_tensors=\"pt\", sampling_rate=sample_rate)\n",
    "with torch.no_grad():\n",
    "  o= model(i.input_values)\n",
    "\n",
    "print(o.keys())\n",
    "print(o.last_hidden_state.shape)\n",
    "print(o.extract_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.last_hidden_state.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aud_list = glob.glob(f'/mnt/c/Users/PCM/Dropbox/span/sub006/2drt/audio/*')[0]\n",
    "# aud_list.split('/')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/wav2vec2-lv-60-espeak-cv-ft\"#\"facebook/wav2vec2-base-960h\"#\"facebook/wav2vec2-large-xlsr-53\"\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n",
    "model = Wav2Vec2Model.from_pretrained(model_name)\n",
    "\n",
    "def create_audio_emds(model, feature_extractor, in_path = '/mnt/c/Users/PCM/Dropbox/span/sub006/2drt/audio', out_path = './datasets/audios'):\n",
    "    aud_list = glob.glob(f'{in_path}/*')\n",
    "    for path in aud_list:\n",
    "        name = path.split('/')[-1].split('.')[0]\n",
    "        input_audio, sample_rate = librosa.load(f\"{in_path}/{name}.wav\",  sr=16000)\n",
    "\n",
    "        i= feature_extractor(input_audio, return_tensors=\"pt\", sampling_rate=sample_rate)\n",
    "        with torch.no_grad():\n",
    "            o = model(i.input_values)\n",
    "        torch.save(o.last_hidden_state, f'{out_path}/{name}.pt')\n",
    "\n",
    "# create_audio_emds(in_path = '/mnt/c/Users/PCM/Dropbox/span/sub006/2drt/audio', out_path = './datasets/audios/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aaa = torch.load('/mnt/c/Users/PCM/Documents/GitHub/SPAN-rtmri/datasets/audios/sub001_2drt_01_vcv1_r2_video.pt')\n",
    "# aaa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = glob.glob('/mnt/c/Users/PCM/Dropbox/span/sub*')\n",
    "for sub in subjects:\n",
    "    print(sub)\n",
    "    create_audio_emds(model, feature_extractor, in_path = f'{sub}/2drt/audio', out_path = './datasets/audios-eng-pho')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "\n",
    "subjects = glob.glob('/mnt/c/Users/PCM/Dropbox/span/sub*')\n",
    "for sub in subjects:\n",
    "    vids = glob.glob(f'{sub}/2drt/video/*')\n",
    "    # audio_foler = f'{sub}/2drt/audio'\n",
    "    # subprocess.call(f'mkdir {audio_foler}', shell=True)\n",
    "    for i in range(len(vids)):\n",
    "        command = f\"ffmpeg -i {vids[i]} -r 50 ./datasets/images/{vids[i].split('/')[-1].split('.')[0]}-%d.png\"\n",
    "        subprocess.call(command, shell=True)\n",
    "        # !ffmpeg -i vids[i] -r 50 f'./datasets/images/{vids[i].split('/')[-1].split('.')[0]}-%d.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample video to frames in 50 fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -i '/mnt/c/Users/PCM/Dropbox/span/sub006/2drt/video/sub006_2drt_01_vcv1_r1_video.mp4' -r 50 './datasets/images/sub006_2drt_01_vcv1_r1_video-%d.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# from imagen_pytorch import Unet, Imagen\n",
    "from imagen_pytorch import Unet, Imagen, ImagenTrainer\n",
    "# from imagen_pytorch.data import Dataset\n",
    "# unet for imagen\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True),\n",
    "    layer_cross_attns = (False, True, True, True)\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = (2, 4, 8, 8),\n",
    "    layer_attns = (False, False, False, True),\n",
    "    layer_cross_attns = (False, False, False, True)\n",
    ")\n",
    "\n",
    "# imagen, which contains the unets above (base unet and super resoluting ones)\n",
    "\n",
    "imagen = Imagen(\n",
    "    text_embed_dim=1024,\n",
    "    unets = (unet1, unet2),\n",
    "    image_sizes = (64, 64),\n",
    "    timesteps = 1000,\n",
    "    cond_drop_prob = 0.1\n",
    ").cuda()\n",
    "\n",
    "# mock images (get a lot of this) and text encodings from large T5\n",
    "\n",
    "# text_embeds = o.extract_features.view(1413,1,512)[:10].cuda #torch.randn(1, 1, 512).cuda()\n",
    "# images = torch.randn(1, 3, 64, 64).cuda()\n",
    "\n",
    "dataset = span75speaker(transform = data_transforms['val'])\n",
    "\n",
    "trainer = ImagenTrainer(\n",
    "    imagen = imagen,\n",
    "    split_valid_from_train = True # whether to split the validation dataset from the training\n",
    ").cuda()\n",
    "trainer.add_train_dataset(dataset, batch_size = 32)\n",
    "\n",
    "for i in range(1,200000):\n",
    "    loss = trainer.train_step(unet_number = 1, max_batch_size = 32)\n",
    "    print(f'loss: {loss}')\n",
    "\n",
    "    if not (i % 50):\n",
    "        valid_loss = trainer.valid_step(unet_number = 1, max_batch_size = 32)\n",
    "        print(f'valid loss: {valid_loss}')\n",
    "\n",
    "    if not (i % 100) and trainer.is_main: # is_main makes sure this can run in distributed\n",
    "        images = trainer.sample(text_embeds = aud.unsqueeze(0), batch_size = 1, return_pil_images = True, stop_at_unet_number=1) # returns List[Image]\n",
    "        images[0].save(f'./sample_log/sample-{i // 100}.png')\n",
    "\n",
    "# feed images into imagen, training each unet in the cascade\n",
    "\n",
    "# for i in (1, 2):\n",
    "#     loss = imagen(images, text_embeds = text_embeds, unet_number = i)\n",
    "#     loss.backward()\n",
    "\n",
    "# do the above for many many many many steps\n",
    "# now you can sample an image based on the text embeddings from the cascading ddpm\n",
    "\n",
    "# images = imagen.sample(texts = [\n",
    "#     'a whale breaching from afar',\n",
    "#     'young girl blowing out candles on her birthday cake',\n",
    "#     'fireworks with blue and green sparkles'\n",
    "# ], cond_scale = 3.)\n",
    "\n",
    "# images.shape # (3, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(imagen.state_dict(), './imagen-span-070823')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet1 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True),\n",
    "    layer_cross_attns = (False, True, True, True)\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = (2, 4, 8, 8),\n",
    "    layer_attns = (False, False, False, True),\n",
    "    layer_cross_attns = (False, False, False, True)\n",
    ")\n",
    "\n",
    "# imagen, which contains the unets above (base unet and super resoluting ones)\n",
    "\n",
    "imagen = Imagen(\n",
    "    text_embed_dim=1024,\n",
    "    unets = (unet1, unet2),\n",
    "    image_sizes = (64, 64),\n",
    "    timesteps = 1000,\n",
    "    cond_drop_prob = 0.1\n",
    ").cuda()\n",
    "\n",
    "imagen.load_state_dict(torch.load('./imagen-span-070823'))\n",
    "imagen.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images = imagen.sample(text_embeds = torch.randn(1, 1, 512).cuda(), cond_scale = 3.)\n",
    "\n",
    "images.shape # (3, 3, 256, 256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
